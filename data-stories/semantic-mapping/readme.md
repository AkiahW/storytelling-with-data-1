# Final assignment: Semantic Mapping

## Overview

For my final project, I attempted to improve upon an approach devised by Huth et al. (2016; paper available [here](https://www.nature.com/articles/nature17637); code available [here](https://github.com/HuthLab/speechmodeltutorial/)) by which researchers systematically mapped the semantic selectivity of various regions across the human neocortex.  Huth et al. (2016) created a vector-space semantic model describing the similarity between features of 10,470 words.  With these features, they built linear models of fMRI data, and fit a regression model to their temporal presence in ten stories to predict real neural activity while participants viewed an eleventh (held-out) story.  

The current approach models words' semantic features as 985-dimensional vectors.  These vector representations were computed based on the rate at which 10,470 unique words occur within 15 words of 985 "basis words," selected from [Wikipedia's List of 1000 basic words](https://simple.wikipedia.org/wiki/Wikipedia:List_of_1000_basic_words). These co-occurence rates were drawn from a massive corpus of Wikipedia articles, popular books, reddit comments, and the story stimuli used in the experiment.  While this semantic modeling approach shares certain basic properties with algorithms like [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis) and [word2vec](https://en.wikipedia.org/wiki/Word2vec), it is somewhat unusual and self-admittedly ad hoc.

Huth et al. (2016) broadly found that most areas within the semantic system represent information about specific semantic domains.  I therefore decided to try applying [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) to their semantic mapping approach in place of the original semantic model.  LDA (like other topic modeling algorithms) assumes that any given text document is comprised of a set of latent themes (or "topics").  Topic models learn these topics from a collection of text documents (similarly based on the co-occurence rate of words) and infers their proportional presence in new documents.  So, for example, if the word "cat" and "dog" commonly occur in the same document, while "money" and "stock" commonly co-occur in different documents, each pair may be predicted to be part of two different topics.  These topics are identified in an unsupervised manner, but it is sometimes possible to assign names to them in a post-hoc way (e.g., "pets" or "finance").  The key feature to LDA (as opposed to other topic models) is the use of a sparse dirichlet prior, embodying the intuition that documents tend to cover only a small set of topics and that topics tend to use a small number of words frequently.

## Required components

The following modules must be installed to run the notebooks in this directory:
via pip:
- `numpy`
- `hypertools`
- `matplotlib`
- `seaborn`
- `tables`
- `scipy`
- `scikit-learn`
- `nibabel`
- `nltk`
- `joblib`
- `h5py`
- `lxml`
- `shapely`
- `html5lib`
via GitHub:
- `pycortex` (`git clone https://github.com/gallantlab/pycortex.git && cd pycortex && python setup.py install`)

You will also need to download the [data files](https://utexas.box.com/shared/static/4n3lemyec0wlj5rcr80991nxwflsbks9.zip) from Huth et al. (2016)'s experiment and place them in the `data/` folder within this directory.

## A note on running the code

These notebooks take quite a while to run and consume a fair amount of processing resources (particularly fitting the regression models) and memory resources (particularly fitting the topic models).  LDA models fit to more topics and smaller text windows take longer to run (sometimes up to 12 hours).  It is recommended you only run a single notebook at a time and run the pre-written cells that save the fit LDA models as `.joblib` files to having to re-train them each time.  

## Analyses conducted and results

Each notebook beginning with `compare-models` replicates Huth et al. (2016)'s analysis pipeline using both their semantic model and a Latent Dirichlet Allocation model with certain parameters that differ across notebooks.  They constitute assorted examples of the many different approaches I explored fo rthis project, and are set up as follows:
- First, the LDA model is fit, and the Huth model and story dataset is loaded.  Each word from each story is projected into the two models' semantic spaces and their vector representations downsampled to the temporal scale of the fMRI data using a 3-lobe Lanczos filter.  
- Each model's feature representations are then vertically concatenated across stories (after z-scoring each feature within each story to account for the z-scoring of fMRI responses during pre-processing, and trimming five TRs off each end of the stories to account for noise).  
- Next four delays of each stimulus representation are concatenated and a linear finite impulse response model is created for each semantic feature (for each voxel).  This serves to estimate the slow hemodynamic response function through which fMRI scans measure neural activity.  
- Finally, the real fMRI response data of participants listening to these stories is loaded, and a linear regression model is fit to predict the response of each voxel from a weighted sum of the stimuli's semantic features.  The regression model is fit to ten of the eleven stories, while the remaining one is used to cross-validate the model's success. A single function pre-written by Huth et al. (2016) packages multiple rounds of this fitting with optimizing the hyperparameter &alpha; by maximizing the mean correlation between small chunks of predicted and real neural responses to the eleventh story.  I conducted this analysis step on the semantic features of the stories in the two spaces, separately.
- Finally, I compare the successes of the two modeling approaches by plotting the distribution of correlations between predicted and actual responses for each voxel in the brain, along with the median of each distribution (the distributions tended to be skewed left).

I created the remaining notebook, `search_params.ipynb`, after attempting this procedure with a number of different parameter combinations to the LDA, all of which failed to predict voxel responses with the accuracy of the original semantic model.  I first attempted using an LDA model pre-fit to ~3600 Wikipedia articles to predict the topic vector representations of each word from each stimulus story.  However, this model predicted responses with a median correlation of 0.021 (versus the Huth model's median correlation of 0.066).  I investigated further and found that despite the large corpus of documents I used, nearly one third of the words in the story stimuli were not contained within my corpus.  Huth's model, however, included the stories in their semantic model training corpus.  I then repeated the analysis, training the LDA model on the Wikipedia articles _and_ the story stimuli, but found no significant improvement.  I tried for a greater specificity to the stories' use of words by training only on the stories' content, and ended up with a model that performed worse (as the semantic system is of course designed to be generalized in its responses to content across _all_ contexts, and of course not just its specific use case in these stories).  Then, realizing that the Huth semantic model incremented word-relatedness scores by the number of times words occurred within 15 words of each other, I changed my approach from considering each Wikipedia article and story as a single document and considered each window of 15 words as a document to compute co-occurrence.  Unfortunately, this resulted in a training corpus of nearly 650,000 documents and fitting the model to it was simply too slow to attempt more than once.  I also tried various iterations, increasing the size of the windows, changing whether they overlapped or did not, and changing the number of topics the LDA model was set to identify (the Huth model casts words as 985-dimensional vectors, providing far greater specificity in semantic feature variation than the 100 topic dimensions I began by using).  I finally realized I was ultimately shooting in the dark, trying combinations of these parameters, and decided to simplify the problem by performing a grid search.  

`search_params.ipynb` wraps all of the analysis steps into a single function, and runs it on each combination of eight possible numbers of topics and four possible word window sizes.  At the bottom of this notebook, the effect of changing these two parameters is estimated by averaging over the results of all combinations involving each, and the result plotted.  These estimates are extremely rough, but seem to indicate preliminarily that a smaller number of topics leads to better predictions than a larger number.  This is surprising, as this small number sacrifices specificity in semantic representations, and may suggest that a different semantic modeling algorithm may be better suited to this task than LDA.  However, the average success increases again at the top of the range of `n_topics` over which I searched, and I plan to extend the range of this search to examine whether that trend continues.  Smaller word windows also appear to begat greater predictive success than larger ones—potentially because they place more emphasis on the proximity of word co-occurrences than windows that span a greater proportion of the full document—although there seems to be a threshold of size below which success is decreased (possibly because the windows were too small to suppress syntactic effects).

Despite the variation in success between parameter combinations, none of these approaches were able to match the accuracy of Huth et al. (2016)'s semantic model.  The most successful combination (five topics and 200-word windows) achieved a median correlation of ~0.041, compared to the original model's score of ~0.066.  One potential explanation for this is the disparity in training corpus size.  Huth et al. (2016)'s model was based on "a decently large corpus of text (a couple billion words, comprising the stories used as stimuli here, 604 popular books, 2,405,569 wikipedia pages, and 36,333,459 user comments scraped from reddit.com) and a lexicon of roughly 10,000 words."  Meanwhile, the corpus of Wikipedia articles I used totaled ~3600.  In addition to further altering various steps of Huth et al. (2016)'s approach, I hope to improve this work in the future by constructing a comparably large training corpus to see if that impacts success.
